{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO1VKF1x+GZVyDoIZSEksJv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Gradient Accumulation\n","In many situations, we want to have a high batch size (desired batch size), however our GPU can only handle a specific batch size (tolerable batch size). One option is to have multiple GPUs and use distributed data training. But what if only one GPU is available? The solution is gradient accumulation.\n","\n","\n","- Gradient accumulation (summation) is performing **multiple** backwards passes **before** updating the parameters. The goal is to have the same model parameters for multiple inputs (batches) and then update the model's parameters based on all these batches, instead of performing an update after every single batch. So we run each torelarbale batch size individually with the same model parameters and calculate the gradients without updating the model. When the desired batch size is reached, we can then update the gradients.\n","\n","- Point of confusion. The **computational graph** is automatically destroyed when .backward() is called (unless retain_graph=True is specified), and **NOT** the gradients. The gradients are only reset when calling optimizer.zero_grad()\n"],"metadata":{"id":"lbam1kDG1Bd0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEvSwHtr0iAl"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision"]},{"cell_type":"code","source":["model = torchvision.models.resnet101()\n","num_iterations = 10\n","xe = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"],"metadata":{"id":"nEL_G_M604lM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 50\n","for i in range(num_iterations):\n","  inputs = torch.randn(batch_size, 3, 224, 224)\n","  labels = torch.LongTensor(batch_size).random_(0, 100)\n","  loss = xe(model(inputs), labels)\n","  loss.backward()\n","  optimizer.step()\n","  optimizer.zero_grad()\n","  print('One batch')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bP-25xjJ04nZ","executionInfo":{"status":"ok","timestamp":1688457331763,"user_tz":-330,"elapsed":51734,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}},"outputId":"949a6d89-e532-4d0c-8fdc-ebda99f09198"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["One batch\n","One batch\n","One batch\n","One batch\n","One batch\n","One batch\n","One batch\n","One batch\n","One batch\n","One batch\n"]}]},{"cell_type":"code","source":["desired_batch_size = 100\n","tolerable_batch_size = 50\n","accum_steps = desired_batch_size / tolerable_batch_size"],"metadata":{"id":"CgYBCZGP04pi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(num_iterations):\n","  inputs = torch.randn(tolerable_batch_size, 3, 224, 224)\n","  labels = torch.LongTensor(tolerable_batch_size).random_(0, 100)\n","  loss = xe(model(inputs), labels)\n","  loss = loss / accum_steps\n","  loss.backward()\n","\n","  if ((i + 1) % accum_steps == 0) or ((i + 1) == num_iterations):\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    print('One Batch')"],"metadata":{"id":"wqaGxT8z04rt"},"execution_count":null,"outputs":[]}]}