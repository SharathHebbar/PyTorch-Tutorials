{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"14uEmFqnkXU3nPuB9Q3J8qCrrRrqae7b2","authorship_tag":"ABX9TyPJtgu0tgUooof365A0WjJV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"1rdS5fhDl1AE","executionInfo":{"status":"ok","timestamp":1688138288241,"user_tz":-330,"elapsed":3541,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils import clip_grad_norm"]},{"cell_type":"code","source":["class Dictionary(object):\n","  def __init__(self):\n","    self.word2idx = {}\n","    self.idx2word = {}\n","    self.idx = 0\n","\n","  def add_word(self, word):\n","    if word not in self.word2idx:\n","      self.word2idx[word] = self.idx\n","      self.idx2word[self.idx] = word\n","      self.idx += 1\n","\n","  def __len__(self):\n","    return len(self.word2idx)"],"metadata":{"id":"yE_ysE7tl7fK","executionInfo":{"status":"ok","timestamp":1688138288242,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class TextProcess(object):\n","  def __init__(self):\n","    self.dictionary = Dictionary()\n","\n","  def get_data(self, path, batch_size=20):\n","    with open(path, 'r') as f:\n","      tokens = 0\n","      for line in f:\n","        words = line.split() + ['<eos>']\n","        tokens += len(words)\n","        for word in words:\n","          self.dictionary.add_word(word)\n","   # Create a 1-D tensor that contains the index of all the words in the file\n","    rep_tensor = torch.LongTensor(tokens)\n","    index = 0\n","    with open(path, 'r') as f:\n","      for line in f:\n","        words = line.split() + ['<eos>']\n","        for word in words:\n","          rep_tensor[index] = self.dictionary.word2idx[word]\n","          index += 1\n","\n","    # Batches\n","    num_batches = rep_tensor.shape[0] // batch_size\n","    rep_tensor = rep_tensor[:num_batches * batch_size]\n","    rep_tensor = rep_tensor.view(batch_size, -1)\n","    return rep_tensor\n"],"metadata":{"id":"uP-mQla1l7hu","executionInfo":{"status":"ok","timestamp":1688138288242,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","embed_size = 128 # Input features\n","hidden_size = 1024 # Number of LSTM layers\n","num_layers = 1\n","num_epochs = 20\n","batch_size = 20\n","timesteps = 30\n","learning_rate = 0.2"],"metadata":{"id":"7BwpuE_zl7kS","executionInfo":{"status":"ok","timestamp":1688138288242,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["corpus = TextProcess()\n","corpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ijJxVXOczLIc","executionInfo":{"status":"ok","timestamp":1688138288243,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}},"outputId":"4b75e5f6-e65e-43c0-f4cf-bcd9c6f8a514"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.TextProcess at 0x7f6a7788fd90>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["rep_tensor = corpus.get_data('/content/drive/Shareddrives/Project/NN course/alice.txt')\n","rep_tensor.shape"],"metadata":{"id":"PZT7Wl_el7mY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688138290873,"user_tz":-330,"elapsed":2634,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}},"outputId":"763dcb8b-72bf-4eaa-a53c-0fc583e99e89"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 1484])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["rep_tensor"],"metadata":{"id":"Q-m_DNNel7ov","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688138290874,"user_tz":-330,"elapsed":26,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}},"outputId":"315fd77c-c7ad-4b96-f4e5-d678429a9109"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[   0,    1,    2,  ...,  203,  571,    5],\n","        [ 572,    5,    5,  ...,  988,    5,  107],\n","        [ 117,    3,  609,  ..., 1364, 1010, 1106],\n","        ...,\n","        [   3, 3779,    7,  ...,    5,    5, 2412],\n","        [ 218,   13,    3,  ..., 1286,  112, 5066],\n","        [ 632,    5,  345,  ...,    3, 5287, 4779]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["vocab_size = len(corpus.dictionary)\n","vocab_size"],"metadata":{"id":"xgimHEyol7q3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688138290875,"user_tz":-330,"elapsed":22,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}},"outputId":"c994705f-0801-44a0-c759-ea97cf3d83bf"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5290"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["num_batches = rep_tensor.shape[1] // timesteps\n","num_batches"],"metadata":{"id":"LS541vx3l7tN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688138290876,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}},"outputId":"1c4c5465-2710-448a-8add-93ef0e9c45fb"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["49"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["class TextGenerator(nn.Module):\n","\n","  def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n","    super(TextGenerator, self).__init__()\n","    self.embed = nn.Embedding(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","    self.linear = nn.Linear(hidden_size, vocab_size)\n","\n","\n","  def forward(self, x, h):\n","    x = self.embed(x) # Word embedding\n","\n","    out, (h, c) = self.lstm(x, h)\n","\n","    out = out.reshape(out.size(0) * out.size(1), out.size(2))\n","    out = self.linear(out)\n","    return out, (h, c)\n","\n"],"metadata":{"id":"SKso7CAJl7vd","executionInfo":{"status":"ok","timestamp":1688138290876,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model = TextGenerator(vocab_size, embed_size, hidden_size, num_layers)\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"],"metadata":{"id":"RYU16gccl7yA","executionInfo":{"status":"ok","timestamp":1688138290877,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["If we have a tensor z, 'z.detach()' returns a tensor that shares the same storage as 'z', but with the computation history forgotten. It doesn't know anything about how it was computed. In other words, we have broken the tensor z away from its past history.\n","Here, we want to perform truncated Backpropagation TBPTT splits the 1,000-long sequence into 50 sequences (say) each of length 20 and treats each sequence of length 20 as a separate training case. This is a sensible approach that can work well in practice, but it is blind to temporal dependencies that span more than 20 timesteps."],"metadata":{"id":"pGTdZ2bb07OY"}},{"cell_type":"code","source":["def detach(states):\n","  return [state.detach() for state in states]\n"],"metadata":{"id":"srnK3zmrl70P","executionInfo":{"status":"ok","timestamp":1688138290878,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sharath S Hebbar","userId":"12626432486155971149"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","  states = (torch.zeros(num_layers, batch_size, hidden_size), torch.zeros(num_layers, batch_size, hidden_size))\n","\n","  for i in range(0, rep_tensor.size(1) - timesteps, timesteps):\n","    inputs = rep_tensor[:, i:i + timesteps]\n","    targets = rep_tensor[:, (i + 1):(i + 1) + timesteps]\n","\n","    outputs, _ = model(inputs, states)\n","    loss = loss_fn(outputs, targets.reshape(-1))\n","\n","    model.zero_grad()\n","    loss.backward()\n","\n","    clip_grad_norm(model.parameters(), 0.5)\n","    optimizer.step()\n","\n","    step = (i + 1) // timesteps\n","    if step % 100 == 0:\n","      print(\"Epoch [{} / {}], Loss: {:.4f}\".format(epoch + 1, num_epochs, loss.item()))\n",""],"metadata":{"id":"ohoPxvIal72k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"11d26436-ec11-4c67-901d-3a5fefa5c332"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-14-3660991edacb>:14: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n","  clip_grad_norm(model.parameters(), 0.5)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1 / 20], Loss: 8.5775\n","Epoch [2 / 20], Loss: 73.0060\n","Epoch [3 / 20], Loss: 59.7756\n","Epoch [4 / 20], Loss: 54.6478\n","Epoch [5 / 20], Loss: 50.6953\n","Epoch [6 / 20], Loss: 51.3594\n","Epoch [7 / 20], Loss: 50.5360\n"]}]},{"cell_type":"code","source":["with torch.no_grad():\n","  with open('results.txt', 'w') as f:\n","    state = (torch.zeros(num_layers, 1, hidden_size), torch.zeros(num_layers, 1, hidden_size))\n","    input = torch.randint(0, vocab_size, (1, )).long().unsqueeze(1)\n","\n","    for i in range(500):\n","      output = model(input, state)\n","      print(output.shape)\n","      prob = output.exp()\n","      word_id = torch.multinomial(prob, num_samples=1).item()\n","      print(word_id)\n","      input.fill_(word_id)\n","\n","      word = corpus.dictionary.idx2word[word_id]\n","      word = '\\n' if word == '<eos>' else word + ' '\n","      f.write(word)\n","\n","      if (i + 1) % 100 == 0:\n","        print(\"Sampled [{} / {}] words and save to {}\".format(i + 1, 500, 'results.txt'))\n",""],"metadata":{"id":"f3SExFXhl746"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yk0XwGDOl77J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f9DFV1Mol79Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-DYN8c3Rl8AA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xGl7YAonl8Do"},"execution_count":null,"outputs":[]}]}